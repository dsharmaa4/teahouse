= Conference Script

== Before the presentation

* Prepare for conference `./scripts/get_ready_for_conference.sh`

== Presentation

* Explain the code structure - including Sleuth & Micrometer
** We're using `spring-boot-actuator` to bring in Micrometer and metrics support
** We're using `spring-cloud-sleuth` to give distributed tracing and context propagation and log correlation
** We're using `spring-cloud-sleuth-zipkin` to send tracing information in a Zipkin compatible format
* Show the UI
* Exception example (`english breakfast` gone wrong)
** Got notification on my phone that users are experiencing errors
** METRICS
*** Write a query in prometheus to see if that's actually the case (`rate(http_server_requests_seconds_count{exception!~"None", application="tea-service", uri!~"/actuator/prometheus"}[$__rate_interval])`)
*** We will see a raise in such results for the URL `/tea/\{name}`
** TRACES
*** Now that we see a pattern in behaviour, we can try to narrow down what's going on
*** Go to TEMPO, go to `Query` and for app` tea-service` find logs with severity `ERROR`
**** We can explain that we're going through all the logs that got aggregated and since for a single business transaction we share an identifier called Trace ID we can then visualize the traces
*** Now, pick a Trace ID to view the whole Trace
*** Analyze the trace by going one by one through spans
**** Explain the status codes, the parameters in the database, result set sizes (we will show that for `english breakfast` we get exceptions cause there are entries in the database missing)
**** *SUMMARY* - make a short break to summarize what we've managed to do so far
** LOGS
*** We need to know if this was a single scenario or is it a more frequent scenario?
*** Find all log entries for any app with ERROR log and line containing `english%20breakfast`
*** We will see a pattern where there are plenty of logs related to this entry
** Time to fix this
*** We have found out the following pieces of information
**** Our metrics are confirming that we have a steady amount of error entries on one of our endpoints
**** Our traces showed us that there is an entry in the database to retrieve a list of tea leaves for english breakfast missing
**** Our logs are providing additional context and prove that the problem is repeatable
*** We need to add the missing entry in the database
**** Let's go to the `tealeaf-service` and since we're using Flyway (we're versioning schema changes) we will add a new changeset where we'll add 1 more tealeaf entry for `english breakfast`
**** Run from the IDE another instance of `tealeaf-service` - it will update the schema
**** *SUMMARY* - make a short break to explain why this will work for the other instance - they share a database
**** Check the metrics again to see if number of error status codes is falling
**** Show that there should be no more ERROR status codes in the logs
**** Take a random trace for `english breakfast` and see that result set count is 1
**** Show the dashboard where for a given trace id you can see traces, metrics and logs together
**** Add an alert that when number of http codes is going up we will be alerted
***** Go to Grafana dashboards, pick the Tea API dashboard
***** Modify it and create an alert that if the rate for responses with `exception != "None"` is above 0 we should be notified
* Latency example
** Run the `./scripts/add_latency_to_toxi_proxy.sh` script to introduce latency for the database
** Start talking about how we've managed to save the day and finally will be able to chill out
** Look at the custom Tea Dashboard graphs again and see that something is wrong - we're seeing a big latency increase
** Let's confirm that the latency is there on the API side - let's check Metrics
*** In Boot dashboard for `tea-service` nothing other than the HTTP response time has increased
*** In Boot dashboard for `water-service` nothing has changed
*** In Boot dashboard for `tealeaf-service` there are some changes in database connection
*** In JDBC dashboard for `tealeaf-service` there are some major latency issues with the database
*** Let's look at the custom Tea Dashboard again to use Exemplars to verify how things looked like before and after
**** Check a trace via exemplars for one trace that was working well, then let's look at another one when latency increased - use the split view
**** We see that one of the databases has increased latency!
**** *SUMMARY* - make a short break to explain why we see that the culprit is the database
*** How can we fix this?
**** If we want to quickly provide a solution one way would be to add `@EnableCaching` in `tealeaf-service` and `@Cacheable("tealeaves")` on the Spring Data Repository interface.
**** Start the app with port `8192`
**** Normally we would gracefully de-register the other instance in Eureka wait for app caches to get updated etc. but it's a demo so we won't do it. We will kill the running `8092` app manually and run a second instance with the cache
**** After a while we will see an improvement in latency
**** In the meantime...
***** Show that could use the `spring_data_repository_invocations_seconds_count` with `rate->sum` that the time spent on Spring Data repositories dropped to 0. We could put an alert on the hikari database metrics.
***** Show the LOGS & TRACES & METRICS - as an implementation of the observability vision
* Summarize what we've done
** Exception scenario
*** We've managed to find the reason for an exception by looking at the metrics, traces and logs
*** We've fixed that problem by adding a missing database entry and created an alert so that we get notified if things go wrong
*** We also doubled checked via the metrics that everything is ok
** Latency scenario
*** We've used metrics to confirm that there's a latency issue
*** Using metrics and tracing we've found out where the concrete issue was - somewhere in the database
*** We've filed a ticket to the database team but as a immediate steps we've decided to add caching to our application on the repository side
*** Finally, we've added an alert to catch these issues
* Go back to the slides to do the summary
